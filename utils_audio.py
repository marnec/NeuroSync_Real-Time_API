"""
Utility functions for audio processing.
These functions are called by the routes defined in main.py.
"""

import numpy as np
from io import BytesIO
from base64 import b64decode
import soundfile as sf
import resampy

from neurosync.generate_face_shapes import generate_facial_data_from_bytes

def trim_and_fade(audio, sample_rate, threshold=0.01, fade_duration=0.05):
    above_threshold = np.where(np.abs(audio) > threshold)[0]
    if above_threshold.size == 0:
        return audio

    start_idx = above_threshold[0]
    end_idx = above_threshold[-1] + 1  # +1 to include the last sample
    trimmed_audio = audio[start_idx:end_idx]

    # Calculate number of samples for the fade.
    fade_samples = int(fade_duration * sample_rate)
    fade_samples = min(fade_samples, len(trimmed_audio) // 2)

    fade_in = np.linspace(0, 1, fade_samples)
    fade_out = np.linspace(1, 0, fade_samples)

    trimmed_audio[:fade_samples] *= fade_in
    trimmed_audio[-fade_samples:] *= fade_out

    return trimmed_audio

def generate_speech_segment_tts(text, tts_pipeline, tts_lock):
    """
    Generate a speech segment using the provided TTS pipeline and lock.
    :param text: The input text to synthesize.
    :param tts_pipeline: The TTS pipeline (e.g., Kokoro) loaded from model_loader.
    :param tts_lock: A threading lock to ensure thread safety.
    :return: The synthesized audio as bytes (WAV format) or None on failure.
    """
    try:
        if not text.strip():
            print("Input text is empty or whitespace.")
            return None

        with tts_lock:
            generator = tts_pipeline(
                text, 
                voice='af_bella',  # Change voice here if needed
                speed=0.8, 
                split_pattern=r'\n+'
            )
            audio_segments = []
            for i, (gs, ps, audio) in enumerate(generator):
                # Uncomment below for debug info:
                # print(f"TTS segment {i}: graphemes: {gs}, phonemes: {ps}")
                audio_segments.append(audio)

            if not audio_segments:
                print("No audio segments generated by TTS pipeline.")
                return None

            full_audio = np.concatenate(audio_segments)
            # Trim silence and apply fade-in/out
            full_audio = trim_and_fade(full_audio, sample_rate=24000, threshold=0.01, fade_duration=0.05)

            # Write the full audio into a buffer as WAV
            buffer = BytesIO()
            sf.write(buffer, full_audio, 24000, format='WAV')
            buffer.seek(0)
            return buffer.getvalue()

    except Exception as e:
        print(f"Error generating speech with TTS for text '{text}': {e}")
        raise

def process_transcription(audio_base64, return_timestamps, transgenerator):
    if not audio_base64:
        raise ValueError("No audio data provided.")
    # Decode the base64 string to bytes and read audio data
    audio_bytes = b64decode(audio_base64)
    audio_input, original_sampling_rate = sf.read(BytesIO(audio_bytes))
    # Convert stereo to mono if needed
    if len(audio_input.shape) == 2:
        audio_input = np.mean(audio_input, axis=1)
    target_sampling_rate = 16000
    if original_sampling_rate != target_sampling_rate:
        audio_input = resampy.resample(audio_input, original_sampling_rate, target_sampling_rate)
    audio_duration = len(audio_input) / target_sampling_rate
    if audio_duration > 29:
        return_timestamps = True
    transcription = transgenerator(audio_input, return_timestamps=return_timestamps)
    if transcription and isinstance(transcription, dict):
        transcription_text = transcription.get('text', '').strip()
        result = {"status": "success", "transcription": transcription_text}
        if return_timestamps:
            result['timestamps'] = transcription.get('segments', [])
        return result
    else:
        raise ValueError("Transcription failed or unexpected format.")

def process_blendshapes(audio_bytes, blendshape_model, device, config):
    if not audio_bytes:
        raise ValueError("No audio data provided.")
    generated_facial_data = generate_facial_data_from_bytes(audio_bytes, blendshape_model, device, config)
    if isinstance(generated_facial_data, np.ndarray):
        return {'blendshapes': generated_facial_data.tolist()}
    else:
        return {'blendshapes': generated_facial_data}
